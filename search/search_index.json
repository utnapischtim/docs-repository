{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome to the TU Graz Repository! TU Graz Repository is a ready-to-go, turn-key Research Data Management repository and an instance of InvenioRDM . Development \u00b6 Please visit the invenioRDM documentation for development section. > Getting Started Deployment \u00b6 Get to know current deployment workflow for Tu Graz repository . > Deployment Guides Configs \u00b6 For more configurations such as How the gitlab-runners are registered? How the Database is setup? And so much more... > Configurations","title":"Home"},{"location":"#home","text":"Welcome to the TU Graz Repository! TU Graz Repository is a ready-to-go, turn-key Research Data Management repository and an instance of InvenioRDM .","title":"Home"},{"location":"#development","text":"Please visit the invenioRDM documentation for development section. > Getting Started","title":"Development"},{"location":"#deployment","text":"Get to know current deployment workflow for Tu Graz repository . > Deployment Guides","title":"Deployment"},{"location":"#configs","text":"For more configurations such as How the gitlab-runners are registered? How the Database is setup? And so much more... > Configurations","title":"Configs"},{"location":"configs/","text":"Configurations \u00b6 Configurations and setup of other services. > Gitlab runner > PostgreSQL > SSH-key > CEPH > Override Pages","title":"Configurations"},{"location":"configs/#configurations","text":"Configurations and setup of other services. > Gitlab runner > PostgreSQL > SSH-key > CEPH > Override Pages","title":"Configurations"},{"location":"configs/ceph/","text":"CephFS \u00b6 The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph\u2019s distributed object store, RADOS. CephFS for repository \u00b6 3 Folders were created for each environment: Production environment - Quota 10TB /invenio/invenioprod: client 'fsinvenioprod', Secret in Sesam Test environment - Quota 1TB /invenio/inveniotest: client 'fsinveniotest', Secret in Sesam Dev environment - Quota 1TB /invenio/inveniodev: client 'fsinveniodev', Secret in Sesam In this guideline we will take a look on how the CEPH FS is configired for the production environment. Which is also the same for Dev and Test environments. Steps \u00b6 install ceph-common \u00b6 apt install ceph-common Create a file \u00b6 Create a file, and add the secret from sesam.tugraz nano /etc/ceph/ceph.client.fsinvenioprod Create a mount path \u00b6 Create a directory which will be mounted to the CEPH FS. mkdir /storage Test mount \u00b6 This will temporarily mount the directory `/storage to the /invenio/invenioprod mount -t ceph <ip:port>:/invenio/invenioprod /storage -o name = fsinvenioprod,secretfile = /etc/ceph/ceph.client.fsinvenioprod Check mounted file systems: \u00b6 df -h Filesystem Size Used Avail Use% Mounted on <ip:port>:/invenio/invenioprod size 0 size 0% /storage Unmount \u00b6 After this temporarily mounting works, we will unmount it so later we can configure it properly. umount /storage/ Add Mount configuration \u00b6 open the /etc/fstab file and edit it as below: # <file system> <mount point> <type> <options> <dump> <pass> <ip:port>:/invenio/invenioprod /storage ceph _netdev,name = fsinvenioprod,secretfile = /etc/ceph/ceph.client.fsinvenioprod 0 0 Mount \u00b6 mount /storage","title":"CEPH"},{"location":"configs/ceph/#cephfs","text":"The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph\u2019s distributed object store, RADOS.","title":"CephFS"},{"location":"configs/ceph/#cephfs-for-repository","text":"3 Folders were created for each environment: Production environment - Quota 10TB /invenio/invenioprod: client 'fsinvenioprod', Secret in Sesam Test environment - Quota 1TB /invenio/inveniotest: client 'fsinveniotest', Secret in Sesam Dev environment - Quota 1TB /invenio/inveniodev: client 'fsinveniodev', Secret in Sesam In this guideline we will take a look on how the CEPH FS is configired for the production environment. Which is also the same for Dev and Test environments.","title":"CephFS for repository"},{"location":"configs/ceph/#steps","text":"","title":"Steps"},{"location":"configs/ceph/#install-ceph-common","text":"apt install ceph-common","title":"install ceph-common"},{"location":"configs/ceph/#create-a-file","text":"Create a file, and add the secret from sesam.tugraz nano /etc/ceph/ceph.client.fsinvenioprod","title":"Create a file"},{"location":"configs/ceph/#create-a-mount-path","text":"Create a directory which will be mounted to the CEPH FS. mkdir /storage","title":"Create a mount path"},{"location":"configs/ceph/#test-mount","text":"This will temporarily mount the directory `/storage to the /invenio/invenioprod mount -t ceph <ip:port>:/invenio/invenioprod /storage -o name = fsinvenioprod,secretfile = /etc/ceph/ceph.client.fsinvenioprod","title":"Test mount"},{"location":"configs/ceph/#check-mounted-file-systems","text":"df -h Filesystem Size Used Avail Use% Mounted on <ip:port>:/invenio/invenioprod size 0 size 0% /storage","title":"Check mounted file systems:"},{"location":"configs/ceph/#unmount","text":"After this temporarily mounting works, we will unmount it so later we can configure it properly. umount /storage/","title":"Unmount"},{"location":"configs/ceph/#add-mount-configuration","text":"open the /etc/fstab file and edit it as below: # <file system> <mount point> <type> <options> <dump> <pass> <ip:port>:/invenio/invenioprod /storage ceph _netdev,name = fsinvenioprod,secretfile = /etc/ceph/ceph.client.fsinvenioprod 0 0","title":"Add Mount configuration"},{"location":"configs/ceph/#mount","text":"mount /storage","title":"Mount"},{"location":"configs/gitlab-runner/","text":"Gitlab Runner \u00b6 GitLab Runner is an application that works with GitLab CI/CD to run jobs in a pipeline. Tu Graz Repository has a Gitlab group invenio . That has a group runner Gitlab-Runner-03-Produktion-Invenio-Shell , which is provided by ZID . We are using this group runner to run our Pipeline jobs, from gitlab to our VM server's. How the GitLab runners are registered? \u00b6 Access the node - for example: ssh mojib@instance_prod.tugraz.at then enter your password from sesam.tugraz to gain access to the server. Add GitLab\u2019s official repository . curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash install the gitlab-runner apt install gitlab-runner Register the gitlab-runner for your group runner. gitlab-runner register Input Value gitlab-ci URL: https://gitlab.tugraz.at/ gitlab-ci token: Check sesam.tugraz gitlab-ci description: invenio01-prod gitlab-ci tags: prod-one executor: shell Make sure to give permission. usermod -aG docker gitlab-runner . Runner is registered successfully. Now you should be able to see your Gitlab-runner in the list here . Possible Errors \u00b6 These error might apear when using this registered runner for the first time. The runner throws an ERROR ERROR: Job failed (system failure): prepare environment: exit status 1 SEE HERE . Solution Remove/comment .bash_logout entries. Access your VM server Navigate to /home/gitlab-runner Look for file called .bash_logout Remove or comment the entries on this file Error saving credentials Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY Solution: install package in VM apt install gnupg2 pass . To learn more Gitlab-runner commands, visit the GitLab Runner commands .","title":"Gitlab Runner"},{"location":"configs/gitlab-runner/#gitlab-runner","text":"GitLab Runner is an application that works with GitLab CI/CD to run jobs in a pipeline. Tu Graz Repository has a Gitlab group invenio . That has a group runner Gitlab-Runner-03-Produktion-Invenio-Shell , which is provided by ZID . We are using this group runner to run our Pipeline jobs, from gitlab to our VM server's.","title":"Gitlab Runner"},{"location":"configs/gitlab-runner/#how-the-gitlab-runners-are-registered","text":"Access the node - for example: ssh mojib@instance_prod.tugraz.at then enter your password from sesam.tugraz to gain access to the server. Add GitLab\u2019s official repository . curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash install the gitlab-runner apt install gitlab-runner Register the gitlab-runner for your group runner. gitlab-runner register Input Value gitlab-ci URL: https://gitlab.tugraz.at/ gitlab-ci token: Check sesam.tugraz gitlab-ci description: invenio01-prod gitlab-ci tags: prod-one executor: shell Make sure to give permission. usermod -aG docker gitlab-runner . Runner is registered successfully. Now you should be able to see your Gitlab-runner in the list here .","title":"How the GitLab runners are registered?"},{"location":"configs/gitlab-runner/#possible-errors","text":"These error might apear when using this registered runner for the first time. The runner throws an ERROR ERROR: Job failed (system failure): prepare environment: exit status 1 SEE HERE . Solution Remove/comment .bash_logout entries. Access your VM server Navigate to /home/gitlab-runner Look for file called .bash_logout Remove or comment the entries on this file Error saving credentials Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY Solution: install package in VM apt install gnupg2 pass . To learn more Gitlab-runner commands, visit the GitLab Runner commands .","title":"Possible Errors"},{"location":"configs/override-pages/","text":"Override Pages \u00b6 In some cases, the default pages do not offer all the functionality or modification possibilities needed. It is possible to override the pages, though this should be the last resort as it increases resources needed for maintainability. We will look into how to override some essential pages of invenio-app-rdm with our existing module invenio-theme-tugraz . ! IMPORTANT: This is valid for invenio-app-rdm==v6.0.2 ! Main Search \u00b6 The component to be overridden is search so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/components.js . The original file can be found here: components.js Creating Search App \u00b6 Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/index.js . In here, all the previously defined components will be imported and the search app will be defined. import { createSearchAppInit } from \"@js/invenio_search_ui\" ; import { RDMBucketAggregationElement , RDMRecordFacets , RDMRecordFacetsValues , RDMRecordResultsGridItem , RDMRecordResultsListItem , RDMRecordSearchBarContainer , RDMRecordSearchBarElement , RDMToggleComponent , RDMCountComponent , } from \"./components\" ; const initSearchApp = createSearchAppInit ({ \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"ResultsGrid.item\" : RDMRecordResultsGridItem , \"ResultsList.item\" : RDMRecordResultsListItem , \"SearchApp.facets\" : RDMRecordFacets , \"SearchApp.searchbarContainer\" : RDMRecordSearchBarContainer , \"SearchBar.element\" : RDMRecordSearchBarElement , \"SearchFilters.ToggleComponent\" : RDMToggleComponent , \"Count.element\" : RDMCountComponent , }); Update webpack \u00b6 Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add search component 'invenio-theme-tugraz-rdm-search' : './js/invenio_theme_tugraz/search/index.js' , }, ... Adding Search Template \u00b6 To make use of the new search, a search template will be added. It will import our search app as previously defined in webpack. To achieve this, we will simply copy the current search page from invenio-rdm and replace the standard search app import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/search.html with the content of the base search template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-search.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-search.js'] }} {%- endblock %} Override Config Variable \u00b6 Finally, invenio-rdm must know that it should use our new template for the search. This can be achieved by overriding the config variable SEARCH_UI_SEARCH_TEMPLATE . In invenio_theme_tugraz/config.py add or override the following line: SEARCH_UI_SEARCH_TEMPLATE = \"invenio_theme_tugraz/search.html\" Now invenio-rdm will use the new search template, which will use the new search app. User Record Search \u00b6 The component to be overridden is user_records_search so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/components.js . The original file can be found here: components.js Creating Search App \u00b6 Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/index.js . In here, all the previously defined components will be imported and the search app will be defined. import { createSearchAppInit } from \"@js/invenio_search_ui\" ; import { RDMRecordResultsListItem , RDMRecordResultsGridItem , RDMDepositResults , RDMEmptyResults , RDMUserRecordsSearchLayout , } from \"./components\" ; import { RDMBucketAggregationElement , RDMCountComponent , RDMRecordFacets , RDMRecordFacetsValues , RDMRecordSearchBarElement , RDMToggleComponent , } from \"../search/components\" ; const initSearchApp = createSearchAppInit ({ \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"Count.element\" : RDMCountComponent , \"EmptyResults.element\" : RDMEmptyResults , \"ResultsList.item\" : RDMRecordResultsListItem , \"ResultsGrid.item\" : RDMRecordResultsGridItem , \"SearchApp.facets\" : RDMRecordFacets , \"SearchApp.layout\" : RDMUserRecordsSearchLayout , \"SearchApp.results\" : RDMDepositResults , \"SearchBar.element\" : RDMRecordSearchBarElement , \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"SearchFilters.ToggleComponent\" : RDMToggleComponent , }); Update webpack \u00b6 Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add user record search component 'invenio-theme-tugraz-rdm-user-records-search' : './js/invenio_theme_tugraz/user_records_search/index.js' , }, ... Adding Search Template \u00b6 To make use of the new search, a search template will be added. It will import our search app as previously defined in webpack. To achieve this, we will simply copy the current search page from invenio-rdm and replace the standard search app import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/search_deposit.html with the content of the base user records search template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-user-records-search.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-user-records-search.js'] }} {%- endblock javascript %} Adding Render Function \u00b6 Next, in invenio_theme_tugraz/deposits.py we define a function which will render the previously created template and pass the searchbar_config as argument. from flask import render_template from flask_login import login_required from invenio_app_rdm.records_ui.views.deposits import get_search_url @login_required def deposit_search (): \"\"\"List of user deposits page.\"\"\" return render_template ( \"invenio_theme_tugraz/records/search_deposit.html\" , searchbar_config = dict ( searchUrl = get_search_url ()), ) Adding URL Route \u00b6 All that is left is to add a URL rule, so that the new function is called instead of the one from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined function from invenio_theme_tugraz.deposits import deposit_search and inside the init_app function add: app . add_url_rule ( \"/uploads\" , \"deposit_search\" , deposit_search ) Now invenio-rdm will use the new user record search template, which will use the new search app. Record Landing Page \u00b6 Since we do not need to add additional functionality to the record landing page, there is no need for new components or updating the webpack file. We will simply add and modify the html file and add the URL route. If there is need for additional functionality, follow the steps from the search page guide. Adding Detail Template \u00b6 Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/detail.html with the content of the base record detail template from invenio-app-rdm . Modify the template file as you see fit. Adding Render Function \u00b6 Next, in invenio_theme_tugraz/deposits.py we define a function which will render the previously created template and pass arguments needed by the template. from flask import render_template from invenio_app_rdm.records_ui.views.decorators import ( pass_is_preview , pass_record_files , pass_record_or_draft , ) from invenio_rdm_records.resources.serializers import UIJSONSerializer @pass_is_preview @pass_record_files @pass_record_or_draft def record_detail ( record = None , files = None , pid_value = None , is_preview = False ): \"\"\"Record detail page (aka landing page).\"\"\" files_dict = None if files is None else files . to_dict () return render_template ( \"invenio_theme_tugraz/records/detail.html\" , record = UIJSONSerializer () . serialize_object_to_dict ( record . to_dict ()), pid = pid_value , files = files_dict , permissions = record . has_permissions_to ([ 'edit' , 'new_version' , 'manage' , 'update_draft' , 'read_files' ]), is_preview = is_preview , ) Adding URL Route \u00b6 All that is left is to add a URL rule, so that the new function is called instead of the one from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined function from invenio_theme_tugraz.deposits import record_detail and inside the init_app function add: app . add_url_rule ( \"/records/<pid_value>\" , \"record_detail\" , record_detail ) Now invenio-rdm will use the new record detail template when displaying a record. Deposit Page \u00b6 The component to be overridden is deposit so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/RDMDepositForm.js . The original file can be found here: RDMDepositForm.js Rendering Deposit Form \u00b6 Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/index.js . In here, all the previously defined components will be imported and the deposit form will be rendered. import React from \"react\" ; import ReactDOM from \"react-dom\" ; import \"semantic-ui-css/semantic.min.css\" ; import { getInputFromDOM } from \"react-invenio-deposit\" ; import { RDMDepositForm } from \"./RDMDepositForm\" ; ReactDOM . render ( < RDMDepositForm record = { getInputFromDOM ( \"deposits-record\" )} files = { getInputFromDOM ( \"deposits-record-files\" )} config = { getInputFromDOM ( \"deposits-config\" )} permissions = { getInputFromDOM ( \"deposits-record-permissions\" )} /> , document . getElementById ( \"deposit-form\" ) ); Update webpack \u00b6 Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add deposit component 'invenio-theme-tugraz-rdm-deposit' : './js/invenio_theme_tugraz/deposit/index.js' , }, ... Adding Deposit Template \u00b6 To make use of the new deposit form, a template will be added. It will import the deposit form as previously defined in webpack. To achieve this, we will simply copy the current deposit page from invenio-rdm and replace the standard deposit form import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/deposit.html with the content of the base deposit form template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-deposit.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-deposit.js'] }} {%- endblock %} Adding Render Function \u00b6 Next, in invenio_theme_tugraz/deposits.py we define two functions which will render the previously created template and pass arguments needed by the template. First the create function, which is called when the user wants to create a new record: from flask import render_template from flask_login import login_required from invenio_app_rdm.records_ui.views.decorators import ( pass_draft , pass_draft_files , ) from invenio_app_rdm.records_ui.views.deposits import ( get_form_config , get_search_url , new_record , ) from invenio_rdm_records.resources.serializers import UIJSONSerializer @login_required def deposit_create (): \"\"\"Create a new deposit.\"\"\" return render_template ( \"invenio_theme_tugraz/records/deposit.html\" , forms_config = get_form_config ( createUrl = ( \"/api/records\" )), searchbar_config = dict ( searchUrl = get_search_url ()), record = new_record (), files = dict ( default_preview = None , entries = [], links = {} ), ) Second the edit function, which is called when the user wants to edit an existing record: @login_required @pass_draft @pass_draft_files def deposit_edit ( draft = None , draft_files = None , pid_value = None ): \"\"\"Edit an existing deposit.\"\"\" record = UIJSONSerializer () . serialize_object_to_dict ( draft . to_dict ()) return render_template ( \"invenio_theme_tugraz/records/deposit.html\" , forms_config = get_form_config ( apiUrl = f \"/api/records/ { pid_value } /draft\" ), record = record , files = draft_files . to_dict (), searchbar_config = dict ( searchUrl = get_search_url ()), permissions = draft . has_permissions_to ([ 'new_version' ]) ) Adding URL Route \u00b6 All that is left is to add URL rules, so that the new functions are called instead of the ones from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined functions from invenio_theme_tugraz.deposits import deposit_create , deposit_edit and inside the init_app function add: For new records: app . add_url_rule ( \"/uploads/new\" , \"deposit_create\" , deposit_create ) For editing existing records: app . add_url_rule ( \"/uploads/<pid_value>\" , \"deposit_edit\" , deposit_edit ) ` Now invenio-rdm will use the new deposit form, when creating a new record or updating an existing record. Troubleshoot \u00b6 ManifestKeyNotFoundError \u00b6 check if the spelling of the webpack names matches with the import in the template. reinstall invenio-theme-tugraz In order to pick up all the new files and to rebuild webpack, it is necessary to install invenio-theme-tugraz again. If no such changes happened, this step can be skipped. invenio-cli packages install /path/to/invenio-theme-tugraz","title":"Override Pages"},{"location":"configs/override-pages/#override-pages","text":"In some cases, the default pages do not offer all the functionality or modification possibilities needed. It is possible to override the pages, though this should be the last resort as it increases resources needed for maintainability. We will look into how to override some essential pages of invenio-app-rdm with our existing module invenio-theme-tugraz . ! IMPORTANT: This is valid for invenio-app-rdm==v6.0.2 !","title":"Override Pages"},{"location":"configs/override-pages/#main-search","text":"The component to be overridden is search so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/components.js . The original file can be found here: components.js","title":"Main Search"},{"location":"configs/override-pages/#creating-search-app","text":"Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/search/index.js . In here, all the previously defined components will be imported and the search app will be defined. import { createSearchAppInit } from \"@js/invenio_search_ui\" ; import { RDMBucketAggregationElement , RDMRecordFacets , RDMRecordFacetsValues , RDMRecordResultsGridItem , RDMRecordResultsListItem , RDMRecordSearchBarContainer , RDMRecordSearchBarElement , RDMToggleComponent , RDMCountComponent , } from \"./components\" ; const initSearchApp = createSearchAppInit ({ \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"ResultsGrid.item\" : RDMRecordResultsGridItem , \"ResultsList.item\" : RDMRecordResultsListItem , \"SearchApp.facets\" : RDMRecordFacets , \"SearchApp.searchbarContainer\" : RDMRecordSearchBarContainer , \"SearchBar.element\" : RDMRecordSearchBarElement , \"SearchFilters.ToggleComponent\" : RDMToggleComponent , \"Count.element\" : RDMCountComponent , });","title":"Creating Search App"},{"location":"configs/override-pages/#update-webpack","text":"Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add search component 'invenio-theme-tugraz-rdm-search' : './js/invenio_theme_tugraz/search/index.js' , }, ...","title":"Update webpack"},{"location":"configs/override-pages/#adding-search-template","text":"To make use of the new search, a search template will be added. It will import our search app as previously defined in webpack. To achieve this, we will simply copy the current search page from invenio-rdm and replace the standard search app import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/search.html with the content of the base search template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-search.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-search.js'] }} {%- endblock %}","title":"Adding Search Template"},{"location":"configs/override-pages/#override-config-variable","text":"Finally, invenio-rdm must know that it should use our new template for the search. This can be achieved by overriding the config variable SEARCH_UI_SEARCH_TEMPLATE . In invenio_theme_tugraz/config.py add or override the following line: SEARCH_UI_SEARCH_TEMPLATE = \"invenio_theme_tugraz/search.html\" Now invenio-rdm will use the new search template, which will use the new search app.","title":"Override Config Variable"},{"location":"configs/override-pages/#user-record-search","text":"The component to be overridden is user_records_search so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/components.js . The original file can be found here: components.js","title":"User Record Search"},{"location":"configs/override-pages/#creating-search-app_1","text":"Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/user_records_search/index.js . In here, all the previously defined components will be imported and the search app will be defined. import { createSearchAppInit } from \"@js/invenio_search_ui\" ; import { RDMRecordResultsListItem , RDMRecordResultsGridItem , RDMDepositResults , RDMEmptyResults , RDMUserRecordsSearchLayout , } from \"./components\" ; import { RDMBucketAggregationElement , RDMCountComponent , RDMRecordFacets , RDMRecordFacetsValues , RDMRecordSearchBarElement , RDMToggleComponent , } from \"../search/components\" ; const initSearchApp = createSearchAppInit ({ \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"Count.element\" : RDMCountComponent , \"EmptyResults.element\" : RDMEmptyResults , \"ResultsList.item\" : RDMRecordResultsListItem , \"ResultsGrid.item\" : RDMRecordResultsGridItem , \"SearchApp.facets\" : RDMRecordFacets , \"SearchApp.layout\" : RDMUserRecordsSearchLayout , \"SearchApp.results\" : RDMDepositResults , \"SearchBar.element\" : RDMRecordSearchBarElement , \"BucketAggregation.element\" : RDMBucketAggregationElement , \"BucketAggregationValues.element\" : RDMRecordFacetsValues , \"SearchFilters.ToggleComponent\" : RDMToggleComponent , });","title":"Creating Search App"},{"location":"configs/override-pages/#update-webpack_1","text":"Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add user record search component 'invenio-theme-tugraz-rdm-user-records-search' : './js/invenio_theme_tugraz/user_records_search/index.js' , }, ...","title":"Update webpack"},{"location":"configs/override-pages/#adding-search-template_1","text":"To make use of the new search, a search template will be added. It will import our search app as previously defined in webpack. To achieve this, we will simply copy the current search page from invenio-rdm and replace the standard search app import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/search_deposit.html with the content of the base user records search template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-user-records-search.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-user-records-search.js'] }} {%- endblock javascript %}","title":"Adding Search Template"},{"location":"configs/override-pages/#adding-render-function","text":"Next, in invenio_theme_tugraz/deposits.py we define a function which will render the previously created template and pass the searchbar_config as argument. from flask import render_template from flask_login import login_required from invenio_app_rdm.records_ui.views.deposits import get_search_url @login_required def deposit_search (): \"\"\"List of user deposits page.\"\"\" return render_template ( \"invenio_theme_tugraz/records/search_deposit.html\" , searchbar_config = dict ( searchUrl = get_search_url ()), )","title":"Adding Render Function"},{"location":"configs/override-pages/#adding-url-route","text":"All that is left is to add a URL rule, so that the new function is called instead of the one from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined function from invenio_theme_tugraz.deposits import deposit_search and inside the init_app function add: app . add_url_rule ( \"/uploads\" , \"deposit_search\" , deposit_search ) Now invenio-rdm will use the new user record search template, which will use the new search app.","title":"Adding URL Route"},{"location":"configs/override-pages/#record-landing-page","text":"Since we do not need to add additional functionality to the record landing page, there is no need for new components or updating the webpack file. We will simply add and modify the html file and add the URL route. If there is need for additional functionality, follow the steps from the search page guide.","title":"Record Landing Page"},{"location":"configs/override-pages/#adding-detail-template","text":"Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/detail.html with the content of the base record detail template from invenio-app-rdm . Modify the template file as you see fit.","title":"Adding Detail Template"},{"location":"configs/override-pages/#adding-render-function_1","text":"Next, in invenio_theme_tugraz/deposits.py we define a function which will render the previously created template and pass arguments needed by the template. from flask import render_template from invenio_app_rdm.records_ui.views.decorators import ( pass_is_preview , pass_record_files , pass_record_or_draft , ) from invenio_rdm_records.resources.serializers import UIJSONSerializer @pass_is_preview @pass_record_files @pass_record_or_draft def record_detail ( record = None , files = None , pid_value = None , is_preview = False ): \"\"\"Record detail page (aka landing page).\"\"\" files_dict = None if files is None else files . to_dict () return render_template ( \"invenio_theme_tugraz/records/detail.html\" , record = UIJSONSerializer () . serialize_object_to_dict ( record . to_dict ()), pid = pid_value , files = files_dict , permissions = record . has_permissions_to ([ 'edit' , 'new_version' , 'manage' , 'update_draft' , 'read_files' ]), is_preview = is_preview , )","title":"Adding Render Function"},{"location":"configs/override-pages/#adding-url-route_1","text":"All that is left is to add a URL rule, so that the new function is called instead of the one from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined function from invenio_theme_tugraz.deposits import record_detail and inside the init_app function add: app . add_url_rule ( \"/records/<pid_value>\" , \"record_detail\" , record_detail ) Now invenio-rdm will use the new record detail template when displaying a record.","title":"Adding URL Route"},{"location":"configs/override-pages/#deposit-page","text":"The component to be overridden is deposit so the path for this looks like invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/ . Place the components in invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/RDMDepositForm.js . The original file can be found here: RDMDepositForm.js","title":"Deposit Page"},{"location":"configs/override-pages/#rendering-deposit-form","text":"Now that all components have been created or imported, create the following file invenio_theme_tugraz/assets/semantic-ui/js/invenio_theme_tugraz/deposit/index.js . In here, all the previously defined components will be imported and the deposit form will be rendered. import React from \"react\" ; import ReactDOM from \"react-dom\" ; import \"semantic-ui-css/semantic.min.css\" ; import { getInputFromDOM } from \"react-invenio-deposit\" ; import { RDMDepositForm } from \"./RDMDepositForm\" ; ReactDOM . render ( < RDMDepositForm record = { getInputFromDOM ( \"deposits-record\" )} files = { getInputFromDOM ( \"deposits-record-files\" )} config = { getInputFromDOM ( \"deposits-config\" )} permissions = { getInputFromDOM ( \"deposits-record-permissions\" )} /> , document . getElementById ( \"deposit-form\" ) );","title":"Rendering Deposit Form"},{"location":"configs/override-pages/#update-webpack_2","text":"Now that the functionality is done, it is important to add it to webpack. This makes it accessible to templates. Adding the following line in the invenio_theme_tugraz/webpack.py file, under the semantic-ui entries will do the trick. It will look something like this: \"semantic-ui\" : dict ( entry = { \"invenio-theme-tugraz-theme\" : \"./less/invenio_theme_tugraz/theme.less\" , \"invenio-theme-tugraz-js\" : \"./js/invenio_theme_tugraz/theme.js\" , # add deposit component 'invenio-theme-tugraz-rdm-deposit' : './js/invenio_theme_tugraz/deposit/index.js' , }, ...","title":"Update webpack"},{"location":"configs/override-pages/#adding-deposit-template","text":"To make use of the new deposit form, a template will be added. It will import the deposit form as previously defined in webpack. To achieve this, we will simply copy the current deposit page from invenio-rdm and replace the standard deposit form import with ours. Create the following file invenio_theme_tugraz/templates/invenio_theme_tugraz/records/deposit.html with the content of the base deposit form template from invenio-app-rdm . In that file, all that has to be done is to replace the import: {%- block javascript %} {{ super() }} - {{ webpack['invenio-app-rdm-deposit.js'] }} + {{ webpack['invenio-theme-tugraz-rdm-deposit.js'] }} {%- endblock %}","title":"Adding Deposit Template"},{"location":"configs/override-pages/#adding-render-function_2","text":"Next, in invenio_theme_tugraz/deposits.py we define two functions which will render the previously created template and pass arguments needed by the template. First the create function, which is called when the user wants to create a new record: from flask import render_template from flask_login import login_required from invenio_app_rdm.records_ui.views.decorators import ( pass_draft , pass_draft_files , ) from invenio_app_rdm.records_ui.views.deposits import ( get_form_config , get_search_url , new_record , ) from invenio_rdm_records.resources.serializers import UIJSONSerializer @login_required def deposit_create (): \"\"\"Create a new deposit.\"\"\" return render_template ( \"invenio_theme_tugraz/records/deposit.html\" , forms_config = get_form_config ( createUrl = ( \"/api/records\" )), searchbar_config = dict ( searchUrl = get_search_url ()), record = new_record (), files = dict ( default_preview = None , entries = [], links = {} ), ) Second the edit function, which is called when the user wants to edit an existing record: @login_required @pass_draft @pass_draft_files def deposit_edit ( draft = None , draft_files = None , pid_value = None ): \"\"\"Edit an existing deposit.\"\"\" record = UIJSONSerializer () . serialize_object_to_dict ( draft . to_dict ()) return render_template ( \"invenio_theme_tugraz/records/deposit.html\" , forms_config = get_form_config ( apiUrl = f \"/api/records/ { pid_value } /draft\" ), record = record , files = draft_files . to_dict (), searchbar_config = dict ( searchUrl = get_search_url ()), permissions = draft . has_permissions_to ([ 'new_version' ]) )","title":"Adding Render Function"},{"location":"configs/override-pages/#adding-url-route_2","text":"All that is left is to add URL rules, so that the new functions are called instead of the ones from the base implementation. In invenio_theme_tugraz/ext.py import the previously defined functions from invenio_theme_tugraz.deposits import deposit_create , deposit_edit and inside the init_app function add: For new records: app . add_url_rule ( \"/uploads/new\" , \"deposit_create\" , deposit_create ) For editing existing records: app . add_url_rule ( \"/uploads/<pid_value>\" , \"deposit_edit\" , deposit_edit ) ` Now invenio-rdm will use the new deposit form, when creating a new record or updating an existing record.","title":"Adding URL Route"},{"location":"configs/override-pages/#troubleshoot","text":"","title":"Troubleshoot"},{"location":"configs/override-pages/#manifestkeynotfounderror","text":"check if the spelling of the webpack names matches with the import in the template. reinstall invenio-theme-tugraz In order to pick up all the new files and to rebuild webpack, it is necessary to install invenio-theme-tugraz again. If no such changes happened, this step can be skipped. invenio-cli packages install /path/to/invenio-theme-tugraz","title":"ManifestKeyNotFoundError"},{"location":"configs/postgresql/","text":"PostgreSQL \u00b6 PostgreSQL also known as Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. In this guideline we will take a look on the Postgresql database setup and configurations for Repository Production instance. Using PostgreSQL Version 11 , and VM invenio03-prod.tugraz.at Installation \u00b6 $ apt-get update $ apt-get install postgresql postgresql-contrib By default PostgreSQL has a default user postgres . We can access the database with this user su postgres and access the shell with psql . To change the postgres user password: $ su postgres ( postgres ) $ psql ( postgres ) ALTER USER postgres WITH PASSWORD '************' ; Users and Database \u00b6 Create a admin user: CREATE USER < USERNAME > WITH PASSWORD '************' ; Give the user permissions: ALTER USER \"USERNAME\" WITH SUPERUSER ; Create Database: CREATE database DATABASE_NAME ; Grant permissions of database for created user: GRANT ALL PRIVILEGES ON DATABASE \"DATABASE_NAME\" to USER ; Configuration \u00b6 Allow access \u00b6 Allow remote connection to the webserver VMs. Open the /etc/postgresql/11/main/pg_hba.conf file and edit it as below: # TYPE DATABASE USER ADDRESS METHOD host all <user> ********/28 trust host all <user> ********/28 trust host all <user> repository.tugraz.at trust Listen address \u00b6 Listen to other VMs. Open the /etc/postgresql/11/main/postgresql.conf file and edit it as below: #------------------------------------------------------------------------------ # CONNECTIONS AND AUTHENTICATION #------------------------------------------------------------------------------ # - Connection Settings - - #listen_addresses = 'localhost' # what IP address(es) to listen on; + listen_addresses = '<ip addresses>' # what IP address(es) to listen on; Test connection \u00b6 Access the database from one of the configured machines. psql Make sure you have postgres client installed. psql -U USERNAME -d DATABASE -h HOST_IP Backup & Restore \u00b6 Requirements \u00b6 A server running Linux operating system with PostgreSQL installed. A root password is setup on your server. Backup a Single PostgreSQL Database \u00b6 You will need to use pg_dump tool to backup a PostgreSQL database. This tool will dump all content of a database into a single file. The basic syntax to backup a PostgreSQL database is shown below: pg_dump - U [ option ] [ database_name ] > [ backup_name ] A brief explanation of all available option is shown below: -U : Specify the PostgreSQL username. -W : Force pg_dump command to ask for a password. -F : Specify the format of the output file. -f : Specify the output file. p : Plain text SQL script. c : Specify the custom formate. d : Specify the directory format. t : Specify tar format archive file. For example, create a backup of the PostgreSQL database named db1 in the tar format, run the following command: pg_dump - U postgres - F c db1 > db1 . tar If you want to save the backup in a Plain-text (SQL), run the following command: pg_dump db1 > db1_backup . sql If you want to save the backup in a directory format, run the following command: pg_dump - U postgres - F d db1 > db1_backup If your database is very large and wants to generate a small backup file then you can use pg_dump with a compression tool such as gzip to compress the database backup. pg_dump - U postgres db1 | gzip > db1 . gz You can also reduce the database backup time by dumping number_of_jobs tables simultaneously using the -j flag. pg_dump - U postgres - F d - j 5 db1 - f db1_backup Note : Also keep in mind that the above command will reduce the time of the backup but it also increases the load on the server. Restore a Single PostgreSQL Database \u00b6 If you choose custom, directory, or archive format when taking a database backup. Then, you will need to use pg_restore command to restore your database. The basic syntax to restore a database with pg_restore is shown below: pg_restore - U [ option ] [ db_name ] [ db_backup ] A brief explanation of each option is shown below: -c : Used to drop database objects before recreating them. -C : Used to create a database before restoring into it. -e : Exit if an error has been encountered. -F format : Used to specify the format of the archive. For example, restore a backup from the file db1.tar, you will need to consider two options: If the database already exists. The format of your backup. If your database already exists, you can restore it with the following command: pg_restore - U postgres - Ft - d db1 < db1 . tar If your database is not exists, you can restore it with the following command: pg_restore - U postgres - Ft - C - d db1 < db1 . tar Backup a Remote PostgreSQL Database \u00b6 In order to perform the database backup on the remote PostgreSQL server. You will need to configure your PostgreSQL server to allow remote connection. The basic syntax to backup a remote PostgreSQL database is shown below: pg_dump - h [ remote - postgres - server - ip ] - U [ option ] [ database_name ] > [ backup_name ] For example, create a backup of the PostgreSQL database on the remote server ( 192.168.0.100 ) with name remote_db1 in the tar format, run the following command: pg_dump - h 192 . 168 . 0 . 100 - U postgres - F c remote_db1 > remote_db1 . tar Restore a Remote PostgreSQL Database \u00b6 The basic syntax to restore a remote PostgreSQL database is shown below: pg_restore - h [ remote - postgres - server - ip ] - U [ option ] [ database_name ] < [ backup_name ] For example, restore a database from the file remote_db1.tar on the remote server ( 192.168.0.100 ), run the following command: pg_restore - h 192 . 168 . 0 . 100 - U postgres - Ft - d remote_db1 < remote_db1 . tar Note to restore from plain-text format you need to use psql command instead: psql db1 < db1_backup . sql For more information, you can see the pg_dump and pg_restore reference pages.","title":"PostgreSQL"},{"location":"configs/postgresql/#postgresql","text":"PostgreSQL also known as Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. In this guideline we will take a look on the Postgresql database setup and configurations for Repository Production instance. Using PostgreSQL Version 11 , and VM invenio03-prod.tugraz.at","title":"PostgreSQL"},{"location":"configs/postgresql/#installation","text":"$ apt-get update $ apt-get install postgresql postgresql-contrib By default PostgreSQL has a default user postgres . We can access the database with this user su postgres and access the shell with psql . To change the postgres user password: $ su postgres ( postgres ) $ psql ( postgres ) ALTER USER postgres WITH PASSWORD '************' ;","title":"Installation"},{"location":"configs/postgresql/#users-and-database","text":"Create a admin user: CREATE USER < USERNAME > WITH PASSWORD '************' ; Give the user permissions: ALTER USER \"USERNAME\" WITH SUPERUSER ; Create Database: CREATE database DATABASE_NAME ; Grant permissions of database for created user: GRANT ALL PRIVILEGES ON DATABASE \"DATABASE_NAME\" to USER ;","title":"Users and Database"},{"location":"configs/postgresql/#configuration","text":"","title":"Configuration"},{"location":"configs/postgresql/#allow-access","text":"Allow remote connection to the webserver VMs. Open the /etc/postgresql/11/main/pg_hba.conf file and edit it as below: # TYPE DATABASE USER ADDRESS METHOD host all <user> ********/28 trust host all <user> ********/28 trust host all <user> repository.tugraz.at trust","title":"Allow access"},{"location":"configs/postgresql/#listen-address","text":"Listen to other VMs. Open the /etc/postgresql/11/main/postgresql.conf file and edit it as below: #------------------------------------------------------------------------------ # CONNECTIONS AND AUTHENTICATION #------------------------------------------------------------------------------ # - Connection Settings - - #listen_addresses = 'localhost' # what IP address(es) to listen on; + listen_addresses = '<ip addresses>' # what IP address(es) to listen on;","title":"Listen address"},{"location":"configs/postgresql/#test-connection","text":"Access the database from one of the configured machines. psql Make sure you have postgres client installed. psql -U USERNAME -d DATABASE -h HOST_IP","title":"Test connection"},{"location":"configs/postgresql/#backup-restore","text":"","title":"Backup &amp; Restore"},{"location":"configs/postgresql/#requirements","text":"A server running Linux operating system with PostgreSQL installed. A root password is setup on your server.","title":"Requirements"},{"location":"configs/postgresql/#backup-a-single-postgresql-database","text":"You will need to use pg_dump tool to backup a PostgreSQL database. This tool will dump all content of a database into a single file. The basic syntax to backup a PostgreSQL database is shown below: pg_dump - U [ option ] [ database_name ] > [ backup_name ] A brief explanation of all available option is shown below: -U : Specify the PostgreSQL username. -W : Force pg_dump command to ask for a password. -F : Specify the format of the output file. -f : Specify the output file. p : Plain text SQL script. c : Specify the custom formate. d : Specify the directory format. t : Specify tar format archive file. For example, create a backup of the PostgreSQL database named db1 in the tar format, run the following command: pg_dump - U postgres - F c db1 > db1 . tar If you want to save the backup in a Plain-text (SQL), run the following command: pg_dump db1 > db1_backup . sql If you want to save the backup in a directory format, run the following command: pg_dump - U postgres - F d db1 > db1_backup If your database is very large and wants to generate a small backup file then you can use pg_dump with a compression tool such as gzip to compress the database backup. pg_dump - U postgres db1 | gzip > db1 . gz You can also reduce the database backup time by dumping number_of_jobs tables simultaneously using the -j flag. pg_dump - U postgres - F d - j 5 db1 - f db1_backup Note : Also keep in mind that the above command will reduce the time of the backup but it also increases the load on the server.","title":"Backup a Single PostgreSQL Database"},{"location":"configs/postgresql/#restore-a-single-postgresql-database","text":"If you choose custom, directory, or archive format when taking a database backup. Then, you will need to use pg_restore command to restore your database. The basic syntax to restore a database with pg_restore is shown below: pg_restore - U [ option ] [ db_name ] [ db_backup ] A brief explanation of each option is shown below: -c : Used to drop database objects before recreating them. -C : Used to create a database before restoring into it. -e : Exit if an error has been encountered. -F format : Used to specify the format of the archive. For example, restore a backup from the file db1.tar, you will need to consider two options: If the database already exists. The format of your backup. If your database already exists, you can restore it with the following command: pg_restore - U postgres - Ft - d db1 < db1 . tar If your database is not exists, you can restore it with the following command: pg_restore - U postgres - Ft - C - d db1 < db1 . tar","title":"Restore a Single PostgreSQL Database"},{"location":"configs/postgresql/#backup-a-remote-postgresql-database","text":"In order to perform the database backup on the remote PostgreSQL server. You will need to configure your PostgreSQL server to allow remote connection. The basic syntax to backup a remote PostgreSQL database is shown below: pg_dump - h [ remote - postgres - server - ip ] - U [ option ] [ database_name ] > [ backup_name ] For example, create a backup of the PostgreSQL database on the remote server ( 192.168.0.100 ) with name remote_db1 in the tar format, run the following command: pg_dump - h 192 . 168 . 0 . 100 - U postgres - F c remote_db1 > remote_db1 . tar","title":"Backup a Remote PostgreSQL Database"},{"location":"configs/postgresql/#restore-a-remote-postgresql-database","text":"The basic syntax to restore a remote PostgreSQL database is shown below: pg_restore - h [ remote - postgres - server - ip ] - U [ option ] [ database_name ] < [ backup_name ] For example, restore a database from the file remote_db1.tar on the remote server ( 192.168.0.100 ), run the following command: pg_restore - h 192 . 168 . 0 . 100 - U postgres - Ft - d remote_db1 < remote_db1 . tar Note to restore from plain-text format you need to use psql command instead: psql db1 < db1_backup . sql For more information, you can see the pg_dump and pg_restore reference pages.","title":"Restore a Remote PostgreSQL Database"},{"location":"configs/sshkey/","text":"SSH-key \u00b6 Services VM deployment is using direct ssh commands to reach the VM. In order to know the ssh keys configuration, please take a look into the steps below: Generate ssh-key in VM \u00b6 This command will generate a new ssh-key. $ ssh-keygen Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. Add private key to CI/CD variable \u00b6 we have defined a variable PROD_03_PRIVATE_KEY in our Gitlab invenio group variable . And we are adding the generated private key /root/.ssh/id_rsa to PROD_03_PRIVATE_KEY . authorized_keys \u00b6 The authorized_keys file in SSH specifies the SSH keys that can be used for logging into the user account for which the file is configured. Add public key to authorized_keys: $ cat /root/.ssh/id_rsa.pub >> /.ssh/authorized_keys known_hosts \u00b6 group runner Gitlab-Runner-03-Produktion-Invenio-Shell has a known_hosts that contains a list of public keys for all the hosts which the user has connected to. These list of public keys are in CI/CD variable SSH_SERVER_HOSTKEYS . To add new host: $ ssh-keyscan invenio03-prod.tugraz.at And then update the variable SSH_SERVER_HOSTKEYS .","title":"SSH-key"},{"location":"configs/sshkey/#ssh-key","text":"Services VM deployment is using direct ssh commands to reach the VM. In order to know the ssh keys configuration, please take a look into the steps below:","title":"SSH-key"},{"location":"configs/sshkey/#generate-ssh-key-in-vm","text":"This command will generate a new ssh-key. $ ssh-keygen Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub.","title":"Generate ssh-key in VM"},{"location":"configs/sshkey/#add-private-key-to-cicd-variable","text":"we have defined a variable PROD_03_PRIVATE_KEY in our Gitlab invenio group variable . And we are adding the generated private key /root/.ssh/id_rsa to PROD_03_PRIVATE_KEY .","title":"Add private key to CI/CD variable"},{"location":"configs/sshkey/#authorized_keys","text":"The authorized_keys file in SSH specifies the SSH keys that can be used for logging into the user account for which the file is configured. Add public key to authorized_keys: $ cat /root/.ssh/id_rsa.pub >> /.ssh/authorized_keys","title":"authorized_keys"},{"location":"configs/sshkey/#known_hosts","text":"group runner Gitlab-Runner-03-Produktion-Invenio-Shell has a known_hosts that contains a list of public keys for all the hosts which the user has connected to. These list of public keys are in CI/CD variable SSH_SERVER_HOSTKEYS . To add new host: $ ssh-keyscan invenio03-prod.tugraz.at And then update the variable SSH_SERVER_HOSTKEYS .","title":"known_hosts"},{"location":"deployment/","text":"Infrastructure \u00b6 Tu Graz Repository has 3 instance running on Debian (VMs). \u00b6 > Production Instance > Test Instance > Development Instance","title":"Infrastructure"},{"location":"deployment/#infrastructure","text":"","title":"Infrastructure"},{"location":"deployment/#tu-graz-repository-has-3-instance-running-on-debian-vms","text":"> Production Instance > Test Instance > Development Instance","title":"Tu Graz Repository has 3 instance running on Debian (VMs)."},{"location":"deployment/deploy/","text":"Deploy \u00b6 Ready to deploy your changes in one of our instances: In this guideline, we will take a look at when the pipeline for different instances is executed. Deploy Dev ( Development instance ) Deploy Test ( Test instance ) Deploy Production ( Production instance ) Deploy Dev \u00b6 Every time there is a branch dev or merge_request to the branch master of Repository the pipeline stage for dev is executed. Pipeline \u00b6 Deploy Test \u00b6 Every commit or merge to the master branch of Repository will run Pipeline and deploy the changes to the Test instance . Pipeline \u00b6 Deploy Production \u00b6 Every new Tag/release of the Repository will run Pipeline and deploy the changes to the Production instance . Steps \u00b6 Deployment to production requires a new Tag/release of the Repository . Meaning we should only deploy to production when we have a new Tag/release . 1. Before creating a new Tag/release first we must change the value of TAG_PROD in GitLab CI/CD variables , to our expected new Tag/release version. For example the following change in TAG_PROD variable: - v2.0.2 + v2.0.3 2. Create a new Tag/release for the Repository , using semantic-versioning. for example v2.0.3 , same value as in the TAG_PROD file. For Example using git: git tag -a v2.0.3 -m \"my version 0.1.2\" Pipeline \u00b6","title":"Deploy"},{"location":"deployment/deploy/#deploy","text":"Ready to deploy your changes in one of our instances: In this guideline, we will take a look at when the pipeline for different instances is executed. Deploy Dev ( Development instance ) Deploy Test ( Test instance ) Deploy Production ( Production instance )","title":"Deploy"},{"location":"deployment/deploy/#deploy-dev","text":"Every time there is a branch dev or merge_request to the branch master of Repository the pipeline stage for dev is executed.","title":"Deploy Dev"},{"location":"deployment/deploy/#pipeline","text":"","title":"Pipeline"},{"location":"deployment/deploy/#deploy-test","text":"Every commit or merge to the master branch of Repository will run Pipeline and deploy the changes to the Test instance .","title":"Deploy Test"},{"location":"deployment/deploy/#pipeline_1","text":"","title":"Pipeline"},{"location":"deployment/deploy/#deploy-production","text":"Every new Tag/release of the Repository will run Pipeline and deploy the changes to the Production instance .","title":"Deploy Production"},{"location":"deployment/deploy/#steps","text":"Deployment to production requires a new Tag/release of the Repository . Meaning we should only deploy to production when we have a new Tag/release . 1. Before creating a new Tag/release first we must change the value of TAG_PROD in GitLab CI/CD variables , to our expected new Tag/release version. For example the following change in TAG_PROD variable: - v2.0.2 + v2.0.3 2. Create a new Tag/release for the Repository , using semantic-versioning. for example v2.0.3 , same value as in the TAG_PROD file. For Example using git: git tag -a v2.0.3 -m \"my version 0.1.2\"","title":"Steps"},{"location":"deployment/deploy/#pipeline_2","text":"","title":"Pipeline"},{"location":"deployment/dev/","text":"Coming soon","title":"Development instance"},{"location":"deployment/production/","text":"Production Instance \u00b6 repository.tugraz.at Production instance has 3 VMs: Web Server VM (invenio01-prod) Web Server VM (invenio02-prod) Services VM (invenio03-prod) These virtual machines are split into two categories, Web Servers and Services . In this guideline we will take a look on both: Web Server \u00b6 Base image of the repository instance. uWSGI is a software application that \"aims at developing a full stack for building hosting services\". uwsgi (all lowercase) is the native binary protocol that uWSGI uses to communicate with other servers. Celery is asynchronous task queue or job queue which is based on distributed message passing. While it supports scheduling, its focus is on operations in real time. Alongside our base image, we are also pushing a NGINX container as a front-end proxy. Nginx is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. The web server VMs are configured to the F5 load balancer that is provided by Tu Graz ZID . The F5 load balancer is forwarding the requests to one of the (Web Server VM), and then the requests are proxied by NGINX to UWSGI web applications . As shown below: The gitlab repository Repository is the Web server, which holds all the files and data to build the application's base image and run the docker containers to our instances. The base image is a Docker image that will include all the dependencies required to run the application web-server, and its build with the help of Dockerfile and the commands define in it. From the base image then we are running three containers web-ui , web-api , and a worker with the help of the docker-compose file, as shown below. Alongside these three containers from the base image, the docker-compose is also responsible to push a NGINX container to our servers. which then proxies the requests to the web-ui , and web-api , as shown below: docker-compose \u00b6 docker-compose is a tool for defining and running multi-container Docker applications. docker-compose-prod.yml # -*- coding: utf-8 -*- # # Copyright (C) 2021 Graz University of Technology # # Following services are included: # - Frontend server: Nginx (exposed port: 8080) # - UI application: UWSGI (not exposed) # - API application: UWSGI (not exposed) # - Worker: Celery (not exposed) version : '2.2' services : # Frontend frontend : image : registry.gitlab.tugraz.at/invenio/nginx:prod restart : \"always\" volumes : - static_data:/opt/invenio/var/instance/static links : - web-ui - web-api ports : - \"8080:8080\" # UI Application web-ui : command : [ \"uwsgi /opt/invenio/var/instance/uwsgi_ui.ini\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} env_file : ${ENV_FILE_PROD} ports : - \"5000\" volumes : - static_data:/opt/invenio/var/instance/static - uploaded_data:/opt/invenio/var/instance/data - archived_data:/opt/invenio/var/instance/archive # API Rest Application web-api : command : [ \"uwsgi /opt/invenio/var/instance/uwsgi_rest.ini\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} env_file : ${ENV_FILE_PROD} ports : - \"5000\" volumes : - uploaded_data:/opt/invenio/var/instance/data - archived_data:/opt/invenio/var/instance/archive # Worker worker : restart : \"always\" env_file : ${ENV_FILE_PROD} command : [ \"celery -A invenio_app.celery worker --loglevel=INFO\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} volumes : static_data : uploaded_data : driver : local driver_opts : type : none o : bind device : /storage archived_data : Dockerfile \u00b6 A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. The Dockerfile for Repository , as below. # Dockerfile that builds a fully functional image of your app. # # This image installs all Python dependencies for your application. It's based # on CentOS 8 with Python 3.8 (https://github.com/inveniosoftware/docker-invenio) # and includes Pip, Pipenv, Node.js, NPM and some few standard libraries # Repository usually needs. # Pulling Centos8 with python3.8 image. FROM inveniosoftware/centos8-python:3.8 # env arg ARG ENVIRONMENT # These packages are required for shibboleth authentication. # Installing xmlsec packages RUN yum localinstall -y http://mirror.centos.org/centos/8/PowerTools/x86_64/os/Packages/xmlsec1-devel-1.2.25-4.el8.x86_64.rpm \\ http://mirror.centos.org/centos/8/PowerTools/x86_64/os/Packages/xmlsec1-openssl-devel-1.2.25-4.el8.x86_64.rpm # Installing libtool package RUN yum localinstall -y http://mirror.centos.org/centos/8/AppStream/x86_64/os/Packages/libtool-ltdl-devel-2.4.6-25.el8.x86_64.rpm # Copy Pipfile and pipfile.lock to ./ directory COPY Pipfile Pipfile.lock ./ # Install all the dependecies defined in the Pipfile. RUN pipenv install --deploy --system --pre # Copy other necessary files COPY ./docker/uwsgi/ ${ INVENIO_INSTANCE_PATH } COPY ./prod/invenio.cfg ${ INVENIO_INSTANCE_PATH } COPY ./templates/ ${ INVENIO_INSTANCE_PATH } /templates/ COPY ./app_data/ ${ INVENIO_INSTANCE_PATH } /app_data/ COPY ./ . # This will create, install and build all the statics & assets. RUN cp -r ./static/. ${ INVENIO_INSTANCE_PATH } /static/ && \\ cp -r ./assets/. ${ INVENIO_INSTANCE_PATH } /assets/ && \\ invenio collect --verbose && \\ invenio webpack create && \\ invenio webpack install --unsafe && \\ invenio webpack build # Instruction used to configure how the container will run. ENTRYPOINT [ \"bash\" , \"-c\" ] Pipeline \u00b6 .gitlab-ci.yml \u00b6 is a YAML file that you create on your project's root. This file automatically runs whenever you push code in the repository. Pipelines consist of one or more stages that run in order and can each contain one or more jobs that run in parallel. These jobs (or scripts) get executed by the GitLab Runner agent. Repository production pipeline consist of three stages : build_prod \u00b6 In this stage the pipeline is building base image for Production instance . Login to docker, clean docker cache, for containers, volumes and images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Build and push base image to gitlab Container Registry . Using Tag name CI_COMMIT_TAG , which is a reference to the latest repository tag/release. script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name shell : tags: - shell Full build_prod stage: build_prod: stage: build_prod before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - shell prod_one-deploy \u00b6 In this stage pipeline is deploying the base image to the Web Server(invenio01-prod) . Login to docker, clean docker cache, containers, volumes & images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Run docker-compose, pull images and run containers defined in docker-compose.prod.yml file. script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name prod01 : tags: - prod01 Full prod_one-deploy stage: prod_one-deploy: stage: prod_one before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod01 prod_two-deploy \u00b6 In this stage pipeline is deploying the base image to the Web Server(invenio02-prod) . Login to docker, clean docker cache, containers, volumes & images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Run docker-compose, pull images and run containers defined in docker-compose.prod.yml file. script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name prod02 : tags: - prod02 Full test_two stage: prod_two-deploy: stage: prod_two before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod02 Full .gitlab-ci.yml \u00b6 # Pipline stages stages: - build_prod - prod_one - prod_two build_prod: stage: build_prod before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - shell # Deploy stage for web server 01 prod_one-deploy: stage: prod_one before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod01 # Deploy stage for web server 02 prod_two-deploy: stage: prod_two before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod02 Services \u00b6 TU Graz Repository consist of these services: Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Elasticsearch is developed in Java. PostgreSQL is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. Redis Remote Dictionary Server is an in-memory data structure project implementing a distributed, in-memory key\u2013value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. EXIM4 Exim4 is a Message Transfer Agent (MTA) developed at the University of Cambridge for use on Unix systems connected to the Internet. Exim can be installed in place of sendmail, although its configuration is quite different. RabbitMQ RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol, MQ Telemetry Transport. These services have a seperate repository in the Gitlab Group invenio . Except PostgreSQL other Services deployment are the same. As an Example we will have a look into our Elasticsearch deployment. Elasticsearch \u00b6 Dockerfile \u00b6 A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. The Dockerfile for Elasticsearch , as below. # Pull elasticsearch version 7 From docker.elastic.co/elasticsearch/elasticsearch-oss:7.9.3 # add single-node config RUN echo \"discovery.type: single-node\" >> /usr/share/elasticsearch/config/elasticsearch.yml # define docker User USER elasticsearch deploy-prod.sh \u00b6 Is a helper file for deployment, and used by .gitlab-ci.yml. It contains instruction for docker commands. CONTAINER_NAME = elasticsearch IMAGE_NAME = registry.gitlab.tugraz.at/invenio/elasticsearch:latest echo \"################################################\" echo \"Stopping and removing container with given name...... $CONTAINER_NAME \" docker stop $CONTAINER_NAME || true && docker rm $CONTAINER_NAME || true echo \"################################################\" echo \"Removing image if given name exists... $IMAGE_NAME \" if test ! -z \" $( docker images -q $IMAGE_NAME ) \" ; then echo \"Image exist...\" docker rmi -f $IMAGE_NAME fi echo \"################################################\" echo \"Running new container..............\" docker run --name = \" $CONTAINER_NAME \" \\ --mount type = bind,source = /storage,target = /usr/share/elasticsearch/data \\ --memory 1g -p 9200 :9200 -p 9300 :9300 -d \\ -e bootstrap.memory_lock = true \\ -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\ -e discovery.type = single-node \\ --health-cmd = \"curl --fail localhost:9200/_cluster/health?wait_for_status=green || exit 1\" \\ --health-interval = 30s \\ --health-retries = 5 \\ --health-timeout = 30s \\ --restart = always \\ $IMAGE_NAME echo \"################################################\" echo \"job ended...\" .gitlab-ci.yml \u00b6 Elasticsearch pipeline consist of two stages build \u00b6 Pipeline builds the docker image and pushes it to the Registry. Login to docker: before_script: - echo \"################################\" - echo \"login to docker...\" - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY Build and push docker image: script: - echo \"################################\" - echo \"Building the image...\" - docker build -t \"$CI_REGISTRY_IMAGE\" . - echo \"push image to registry...\" - docker push \"$CI_REGISTRY_IMAGE\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell test \u00b6 In this stage the pipeline pulls the newly created image and creates a container to the VM. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in PROD_03_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"SSH-USER\" - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' - eval $(ssh-agent -s) - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh commands logs in to docker, and runs scripts in deploy-prod.sh to our server. script: - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-test.sh)'\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell prod \u00b6 In this stage the pipeline pulls the newly created image and creates a container to the Production instance. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in PROD_03_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"SSH-USER\" - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' - eval $(ssh-agent -s) - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh commands logs in to docker, and runs scripts in deploy-prod.sh to our server. script: - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-prod.sh)'\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell Full .gitlab-ci.yml \u00b6 stages: - build - prod build: stage: build before_script: - echo \"################################\" - echo \"login to docker...\" - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY script: - echo \"################################\" - echo \"Building the image...\" - docker build -t \"$CI_REGISTRY_IMAGE\" . - echo \"push image to registry...\" - docker push \"$CI_REGISTRY_IMAGE\" only: - master tags: - shell prod: stage: prod before_script: - echo \"################################\" - echo \"SSH-USER\" ## ## Install ssh-agent if not already installed, it is required by Docker. ## (change apt-get to yum if you use an RPM-based image) ## - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' ## ## Run ssh-agent (inside the build environment) ## - eval $(ssh-agent -s) ## ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store ## We're using tr to fix line endings which makes ed25519 keys work ## without extra base64 encoding. ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556 ## - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - ## ## Create the SSH directory and give it the right permissions ## - mkdir -p ~/.ssh - chmod 700 ~/.ssh ## ## Use ssh-keyscan to scan the keys of your private server. Replace gitlab.com ## with your own domain name. You can copy and repeat that command if you have ## more than one server to connect to. ## #- ssh-keyscan invenio03-test.tugraz.at >> ~/.ssh/known_hosts #- chmod 644 ~/.ssh/known_hosts ## ## Alternatively, assuming you created the SSH_SERVER_HOSTKEYS variable ## previously, uncomment the following two lines instead. ## - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script: - echo \"################################\" - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-prod.sh)'\" only: - master tags: - shell","title":"Production instance"},{"location":"deployment/production/#production-instance","text":"repository.tugraz.at Production instance has 3 VMs: Web Server VM (invenio01-prod) Web Server VM (invenio02-prod) Services VM (invenio03-prod) These virtual machines are split into two categories, Web Servers and Services . In this guideline we will take a look on both:","title":"Production Instance"},{"location":"deployment/production/#web-server","text":"Base image of the repository instance. uWSGI is a software application that \"aims at developing a full stack for building hosting services\". uwsgi (all lowercase) is the native binary protocol that uWSGI uses to communicate with other servers. Celery is asynchronous task queue or job queue which is based on distributed message passing. While it supports scheduling, its focus is on operations in real time. Alongside our base image, we are also pushing a NGINX container as a front-end proxy. Nginx is a web server that can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. The web server VMs are configured to the F5 load balancer that is provided by Tu Graz ZID . The F5 load balancer is forwarding the requests to one of the (Web Server VM), and then the requests are proxied by NGINX to UWSGI web applications . As shown below: The gitlab repository Repository is the Web server, which holds all the files and data to build the application's base image and run the docker containers to our instances. The base image is a Docker image that will include all the dependencies required to run the application web-server, and its build with the help of Dockerfile and the commands define in it. From the base image then we are running three containers web-ui , web-api , and a worker with the help of the docker-compose file, as shown below. Alongside these three containers from the base image, the docker-compose is also responsible to push a NGINX container to our servers. which then proxies the requests to the web-ui , and web-api , as shown below:","title":"Web Server"},{"location":"deployment/production/#docker-compose","text":"docker-compose is a tool for defining and running multi-container Docker applications. docker-compose-prod.yml # -*- coding: utf-8 -*- # # Copyright (C) 2021 Graz University of Technology # # Following services are included: # - Frontend server: Nginx (exposed port: 8080) # - UI application: UWSGI (not exposed) # - API application: UWSGI (not exposed) # - Worker: Celery (not exposed) version : '2.2' services : # Frontend frontend : image : registry.gitlab.tugraz.at/invenio/nginx:prod restart : \"always\" volumes : - static_data:/opt/invenio/var/instance/static links : - web-ui - web-api ports : - \"8080:8080\" # UI Application web-ui : command : [ \"uwsgi /opt/invenio/var/instance/uwsgi_ui.ini\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} env_file : ${ENV_FILE_PROD} ports : - \"5000\" volumes : - static_data:/opt/invenio/var/instance/static - uploaded_data:/opt/invenio/var/instance/data - archived_data:/opt/invenio/var/instance/archive # API Rest Application web-api : command : [ \"uwsgi /opt/invenio/var/instance/uwsgi_rest.ini\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} env_file : ${ENV_FILE_PROD} ports : - \"5000\" volumes : - uploaded_data:/opt/invenio/var/instance/data - archived_data:/opt/invenio/var/instance/archive # Worker worker : restart : \"always\" env_file : ${ENV_FILE_PROD} command : [ \"celery -A invenio_app.celery worker --loglevel=INFO\" ] image : registry.gitlab.tugraz.at/invenio/repository:${TAG_PROD} volumes : static_data : uploaded_data : driver : local driver_opts : type : none o : bind device : /storage archived_data :","title":"docker-compose"},{"location":"deployment/production/#dockerfile","text":"A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. The Dockerfile for Repository , as below. # Dockerfile that builds a fully functional image of your app. # # This image installs all Python dependencies for your application. It's based # on CentOS 8 with Python 3.8 (https://github.com/inveniosoftware/docker-invenio) # and includes Pip, Pipenv, Node.js, NPM and some few standard libraries # Repository usually needs. # Pulling Centos8 with python3.8 image. FROM inveniosoftware/centos8-python:3.8 # env arg ARG ENVIRONMENT # These packages are required for shibboleth authentication. # Installing xmlsec packages RUN yum localinstall -y http://mirror.centos.org/centos/8/PowerTools/x86_64/os/Packages/xmlsec1-devel-1.2.25-4.el8.x86_64.rpm \\ http://mirror.centos.org/centos/8/PowerTools/x86_64/os/Packages/xmlsec1-openssl-devel-1.2.25-4.el8.x86_64.rpm # Installing libtool package RUN yum localinstall -y http://mirror.centos.org/centos/8/AppStream/x86_64/os/Packages/libtool-ltdl-devel-2.4.6-25.el8.x86_64.rpm # Copy Pipfile and pipfile.lock to ./ directory COPY Pipfile Pipfile.lock ./ # Install all the dependecies defined in the Pipfile. RUN pipenv install --deploy --system --pre # Copy other necessary files COPY ./docker/uwsgi/ ${ INVENIO_INSTANCE_PATH } COPY ./prod/invenio.cfg ${ INVENIO_INSTANCE_PATH } COPY ./templates/ ${ INVENIO_INSTANCE_PATH } /templates/ COPY ./app_data/ ${ INVENIO_INSTANCE_PATH } /app_data/ COPY ./ . # This will create, install and build all the statics & assets. RUN cp -r ./static/. ${ INVENIO_INSTANCE_PATH } /static/ && \\ cp -r ./assets/. ${ INVENIO_INSTANCE_PATH } /assets/ && \\ invenio collect --verbose && \\ invenio webpack create && \\ invenio webpack install --unsafe && \\ invenio webpack build # Instruction used to configure how the container will run. ENTRYPOINT [ \"bash\" , \"-c\" ]","title":"Dockerfile"},{"location":"deployment/production/#pipeline","text":"","title":"Pipeline"},{"location":"deployment/production/#gitlab-ciyml","text":"is a YAML file that you create on your project's root. This file automatically runs whenever you push code in the repository. Pipelines consist of one or more stages that run in order and can each contain one or more jobs that run in parallel. These jobs (or scripts) get executed by the GitLab Runner agent. Repository production pipeline consist of three stages :","title":".gitlab-ci.yml"},{"location":"deployment/production/#build_prod","text":"In this stage the pipeline is building base image for Production instance . Login to docker, clean docker cache, for containers, volumes and images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Build and push base image to gitlab Container Registry . Using Tag name CI_COMMIT_TAG , which is a reference to the latest repository tag/release. script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name shell : tags: - shell Full build_prod stage: build_prod: stage: build_prod before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - shell","title":"build_prod"},{"location":"deployment/production/#prod_one-deploy","text":"In this stage pipeline is deploying the base image to the Web Server(invenio01-prod) . Login to docker, clean docker cache, containers, volumes & images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Run docker-compose, pull images and run containers defined in docker-compose.prod.yml file. script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name prod01 : tags: - prod01 Full prod_one-deploy stage: prod_one-deploy: stage: prod_one before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod01","title":"prod_one-deploy"},{"location":"deployment/production/#prod_two-deploy","text":"In this stage pipeline is deploying the base image to the Web Server(invenio02-prod) . Login to docker, clean docker cache, containers, volumes & images. before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f Run docker-compose, pull images and run containers defined in docker-compose.prod.yml file. script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d Logout from docker after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" Only run for tags . only: - tags Execute jobs with Gitlab-runner tag name prod02 : tags: - prod02 Full test_two stage: prod_two-deploy: stage: prod_two before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod02","title":"prod_two-deploy"},{"location":"deployment/production/#full-gitlab-ciyml","text":"# Pipline stages stages: - build_prod - prod_one - prod_two build_prod: stage: build_prod before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"Build base image...\" - docker build --no-cache -t \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG --build-arg ENVIRONMENT=PROD . - echo \"Push image to the gitlab registry...\" - docker push \"$CI_REGISTRY_IMAGE\":$CI_COMMIT_TAG after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - shell # Deploy stage for web server 01 prod_one-deploy: stage: prod_one before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod01 # Deploy stage for web server 02 prod_two-deploy: stage: prod_two before_script: - echo \"login to docker...\" - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\" - echo \"Clean docker cache, for containers, volumes and images...\" - docker-compose -f docker-compose.prod.yml down - docker system prune -f --volumes - docker system prune -a -f - docker image prune -f script: - echo \"run docker-compose...\" - docker-compose -f docker-compose.prod.yml up -d after_script: - echo \"Logout from docker...\" - \"docker logout ${CI_REGISTRY}\" only: - tags tags: - prod02","title":"Full .gitlab-ci.yml"},{"location":"deployment/production/#services","text":"TU Graz Repository consist of these services: Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Elasticsearch is developed in Java. PostgreSQL is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. Redis Remote Dictionary Server is an in-memory data structure project implementing a distributed, in-memory key\u2013value database with optional durability. Redis supports different kinds of abstract data structures, such as strings, lists, maps, sets, sorted sets, HyperLogLogs, bitmaps, streams, and spatial indexes. EXIM4 Exim4 is a Message Transfer Agent (MTA) developed at the University of Cambridge for use on Unix systems connected to the Internet. Exim can be installed in place of sendmail, although its configuration is quite different. RabbitMQ RabbitMQ is an open-source message-broker software that originally implemented the Advanced Message Queuing Protocol and has since been extended with a plug-in architecture to support Streaming Text Oriented Messaging Protocol, MQ Telemetry Transport. These services have a seperate repository in the Gitlab Group invenio . Except PostgreSQL other Services deployment are the same. As an Example we will have a look into our Elasticsearch deployment.","title":"Services"},{"location":"deployment/production/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"deployment/production/#dockerfile_1","text":"A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. The Dockerfile for Elasticsearch , as below. # Pull elasticsearch version 7 From docker.elastic.co/elasticsearch/elasticsearch-oss:7.9.3 # add single-node config RUN echo \"discovery.type: single-node\" >> /usr/share/elasticsearch/config/elasticsearch.yml # define docker User USER elasticsearch","title":"Dockerfile"},{"location":"deployment/production/#deploy-prodsh","text":"Is a helper file for deployment, and used by .gitlab-ci.yml. It contains instruction for docker commands. CONTAINER_NAME = elasticsearch IMAGE_NAME = registry.gitlab.tugraz.at/invenio/elasticsearch:latest echo \"################################################\" echo \"Stopping and removing container with given name...... $CONTAINER_NAME \" docker stop $CONTAINER_NAME || true && docker rm $CONTAINER_NAME || true echo \"################################################\" echo \"Removing image if given name exists... $IMAGE_NAME \" if test ! -z \" $( docker images -q $IMAGE_NAME ) \" ; then echo \"Image exist...\" docker rmi -f $IMAGE_NAME fi echo \"################################################\" echo \"Running new container..............\" docker run --name = \" $CONTAINER_NAME \" \\ --mount type = bind,source = /storage,target = /usr/share/elasticsearch/data \\ --memory 1g -p 9200 :9200 -p 9300 :9300 -d \\ -e bootstrap.memory_lock = true \\ -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\ -e discovery.type = single-node \\ --health-cmd = \"curl --fail localhost:9200/_cluster/health?wait_for_status=green || exit 1\" \\ --health-interval = 30s \\ --health-retries = 5 \\ --health-timeout = 30s \\ --restart = always \\ $IMAGE_NAME echo \"################################################\" echo \"job ended...\"","title":"deploy-prod.sh"},{"location":"deployment/production/#gitlab-ciyml_1","text":"Elasticsearch pipeline consist of two stages","title":".gitlab-ci.yml"},{"location":"deployment/production/#build","text":"Pipeline builds the docker image and pushes it to the Registry. Login to docker: before_script: - echo \"################################\" - echo \"login to docker...\" - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY Build and push docker image: script: - echo \"################################\" - echo \"Building the image...\" - docker build -t \"$CI_REGISTRY_IMAGE\" . - echo \"push image to registry...\" - docker push \"$CI_REGISTRY_IMAGE\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell","title":"build"},{"location":"deployment/production/#test","text":"In this stage the pipeline pulls the newly created image and creates a container to the VM. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in PROD_03_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"SSH-USER\" - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' - eval $(ssh-agent -s) - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh commands logs in to docker, and runs scripts in deploy-prod.sh to our server. script: - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-test.sh)'\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell","title":"test"},{"location":"deployment/production/#prod","text":"In this stage the pipeline pulls the newly created image and creates a container to the Production instance. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in PROD_03_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"SSH-USER\" - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' - eval $(ssh-agent -s) - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh commands logs in to docker, and runs scripts in deploy-prod.sh to our server. script: - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-prod.sh)'\" Only run for branch master : only: - master Execute jobs with Gitlab-runner tag name shell : tags: - shell","title":"prod"},{"location":"deployment/production/#full-gitlab-ciyml_1","text":"stages: - build - prod build: stage: build before_script: - echo \"################################\" - echo \"login to docker...\" - docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY script: - echo \"################################\" - echo \"Building the image...\" - docker build -t \"$CI_REGISTRY_IMAGE\" . - echo \"push image to registry...\" - docker push \"$CI_REGISTRY_IMAGE\" only: - master tags: - shell prod: stage: prod before_script: - echo \"################################\" - echo \"SSH-USER\" ## ## Install ssh-agent if not already installed, it is required by Docker. ## (change apt-get to yum if you use an RPM-based image) ## - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client git -y )' ## ## Run ssh-agent (inside the build environment) ## - eval $(ssh-agent -s) ## ## Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store ## We're using tr to fix line endings which makes ed25519 keys work ## without extra base64 encoding. ## https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556 ## - echo \"$PROD_03_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - ## ## Create the SSH directory and give it the right permissions ## - mkdir -p ~/.ssh - chmod 700 ~/.ssh ## ## Use ssh-keyscan to scan the keys of your private server. Replace gitlab.com ## with your own domain name. You can copy and repeat that command if you have ## more than one server to connect to. ## #- ssh-keyscan invenio03-test.tugraz.at >> ~/.ssh/known_hosts #- chmod 644 ~/.ssh/known_hosts ## ## Alternatively, assuming you created the SSH_SERVER_HOSTKEYS variable ## previously, uncomment the following two lines instead. ## - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script: - echo \"################################\" - ssh $PROD_03_DOMAIN \"echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER $CI_REGISTRY --password-stdin\" - ssh $PROD_03_DOMAIN \"eval '$(cat ./deploy-prod.sh)'\" only: - master tags: - shell","title":"Full .gitlab-ci.yml"},{"location":"deployment/test/","text":"Test Instance \u00b6 invenio-test.tugraz.at Test instance has 3 VMs: Web Server VM (invenio01-test) Web Server VM (invenio02-test) Services VM (invenio03-test) Test instance configuration is same as > Production Instance .","title":"Test Instance"},{"location":"deployment/test/#test-instance","text":"invenio-test.tugraz.at Test instance has 3 VMs: Web Server VM (invenio01-test) Web Server VM (invenio02-test) Services VM (invenio03-test) Test instance configuration is same as > Production Instance .","title":"Test Instance"},{"location":"deployment/vms/","text":"VMs update & upgrade \u00b6 Backup Make sure you have a regular backup of VMs set up! In total the repository uses 7 virtual machines, in order to keep the packages up to date, we need to run the update & upgrade commands in regular bases. For this we have created a new repository upgrade-vms , this repository consist of all the scripts and pipeline configuration to run our update & upgrade scripts in specified time and date, for all the VMs. .gitlab-ci.yml \u00b6 is a YAML file that you create on your project's root. This file automatically runs whenever specific intervals are met. This file consits of 7 stage, one stage for each virtual machines: - invenio-dev01 - invenio01-test - invenio02-test - invenio03-test - invenio01-prod - invenio02-prod - invenio03-prod All these above stages have the same identical steps, only the environment variables are different, we will cover one of these stage. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in TEST_01_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"################################\" - echo \"SSH-USER\" - eval $(ssh-agent -s) - echo \"$TEST_01_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh runs scripts in scripts.sh to our server. script: - echo \"################################\" - ssh $TEST_01_DOMAIN \"eval '$(cat ./scripts-test.sh)'\" Only run for branch schedules : only: - schedules Execute jobs with Gitlab-runner tag name shell : tags: - shell Full .gitlab-ci.yml: invenio01-test: stage: invenio01-test before_script: - echo \"################################\" - echo \"SSH-USER\" - eval $(ssh-agent -s) - echo \"$TEST_01_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script: - echo \"################################\" - ssh $TEST_01_DOMAIN \"eval '$(cat ./scripts-test.sh)'\" only: - schedules tags: - shell scripts-().sh file \u00b6 scripts-test.sh & scripts-prod.sh is a helper files for deployment, and used by .gitlab-ci.yml . It contains instruction for update/upgrade commands. And a curl check if nginx server is responding. echo \"################################################\" echo \"Run update & upgrade scripts.\" export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -q -y upgrade echo \"################################################\" echo \"Check if nginx server is running.\" nginxserver = ********:8080 check = $( curl -s -w \"%{http_code}\\n\" -L \" $nginxserver \" -o /dev/null ) if [[ $check == 200 || $check == 403 ]] then # Server is online echo \"Server is online\" exit 0 else # Server is offline or not working correctly echo \"Server is offline or not working correctly\" exit 1 fi Configure root access to VMS. \u00b6 The update & upgrade shell commands require root access to the virtual machines. We are using ssh-keys to access the virtual machines with root permission. Please see the SSH-key configuration for further details. Pipeline schedules \u00b6 Now that we have our scripts and pipeline configuration ready, lets configure Gitlab pipeline schedules . Gitlab pipelines are normally run based on certain conditions being met. For example, when a branch is pushed to repository. Pipeline schedules can be used to also run pipelines at specific intervals. For example: Every month on the 22nd for a certain branch. Once every day. Configuring pipeline schedules \u00b6 To schedule a pipeline for project: Navigate to the project\u2019s CI/CD > Schedules page. Click the New schedule button. Fill in the Schedule a new pipeline form. Click the Save pipeline schedule button. In the Schedules index page you can see a list of the pipelines that are scheduled to run. The next run is automatically calculated by the server GitLab is installed on.","title":"VMs"},{"location":"deployment/vms/#vms-update-upgrade","text":"Backup Make sure you have a regular backup of VMs set up! In total the repository uses 7 virtual machines, in order to keep the packages up to date, we need to run the update & upgrade commands in regular bases. For this we have created a new repository upgrade-vms , this repository consist of all the scripts and pipeline configuration to run our update & upgrade scripts in specified time and date, for all the VMs.","title":"VMs update &amp; upgrade"},{"location":"deployment/vms/#gitlab-ciyml","text":"is a YAML file that you create on your project's root. This file automatically runs whenever specific intervals are met. This file consits of 7 stage, one stage for each virtual machines: - invenio-dev01 - invenio01-test - invenio02-test - invenio03-test - invenio01-prod - invenio02-prod - invenio03-prod All these above stages have the same identical steps, only the environment variables are different, we will cover one of these stage. before_script run followings: Install ssh-agent if not already installed, it is required by Docker. Run ssh-agent (inside the build environment) Add the SSH key stored in TEST_01_PRIVATE_KEY variable to the agent store Create the SSH directory and give it the right permissions scan the keys of your private server from variable SSH_SERVER_HOSTKEYS before_script: - echo \"################################\" - echo \"SSH-USER\" - eval $(ssh-agent -s) - echo \"$TEST_01_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script : using ssh runs scripts in scripts.sh to our server. script: - echo \"################################\" - ssh $TEST_01_DOMAIN \"eval '$(cat ./scripts-test.sh)'\" Only run for branch schedules : only: - schedules Execute jobs with Gitlab-runner tag name shell : tags: - shell Full .gitlab-ci.yml: invenio01-test: stage: invenio01-test before_script: - echo \"################################\" - echo \"SSH-USER\" - eval $(ssh-agent -s) - echo \"$TEST_01_PRIVATE_KEY\" | tr -d '\\r' | ssh-add - - mkdir -p ~/.ssh - chmod 700 ~/.ssh - echo \"$SSH_SERVER_HOSTKEYS\" > ~/.ssh/known_hosts - chmod 644 ~/.ssh/known_hosts script: - echo \"################################\" - ssh $TEST_01_DOMAIN \"eval '$(cat ./scripts-test.sh)'\" only: - schedules tags: - shell","title":".gitlab-ci.yml"},{"location":"deployment/vms/#scripts-sh-file","text":"scripts-test.sh & scripts-prod.sh is a helper files for deployment, and used by .gitlab-ci.yml . It contains instruction for update/upgrade commands. And a curl check if nginx server is responding. echo \"################################################\" echo \"Run update & upgrade scripts.\" export DEBIAN_FRONTEND = noninteractive apt-get update && apt-get -q -y upgrade echo \"################################################\" echo \"Check if nginx server is running.\" nginxserver = ********:8080 check = $( curl -s -w \"%{http_code}\\n\" -L \" $nginxserver \" -o /dev/null ) if [[ $check == 200 || $check == 403 ]] then # Server is online echo \"Server is online\" exit 0 else # Server is offline or not working correctly echo \"Server is offline or not working correctly\" exit 1 fi","title":"scripts-().sh file"},{"location":"deployment/vms/#configure-root-access-to-vms","text":"The update & upgrade shell commands require root access to the virtual machines. We are using ssh-keys to access the virtual machines with root permission. Please see the SSH-key configuration for further details.","title":"Configure root access to VMS."},{"location":"deployment/vms/#pipeline-schedules","text":"Now that we have our scripts and pipeline configuration ready, lets configure Gitlab pipeline schedules . Gitlab pipelines are normally run based on certain conditions being met. For example, when a branch is pushed to repository. Pipeline schedules can be used to also run pipelines at specific intervals. For example: Every month on the 22nd for a certain branch. Once every day.","title":"Pipeline schedules"},{"location":"deployment/vms/#configuring-pipeline-schedules","text":"To schedule a pipeline for project: Navigate to the project\u2019s CI/CD > Schedules page. Click the New schedule button. Fill in the Schedule a new pipeline form. Click the Save pipeline schedule button. In the Schedules index page you can see a list of the pipelines that are scheduled to run. The next run is automatically calculated by the server GitLab is installed on.","title":"Configuring pipeline schedules"},{"location":"services/","text":"Services \u00b6 > pgAdmin","title":"Services"},{"location":"services/#services","text":"> pgAdmin","title":"Services"},{"location":"services/pgadmin/","text":"pgAdmin \u00b6 The pgAdmin package is a free and open-source graphical user interface (GUI) administration tool for PostgreSQL, which is configured as a service for your development. docker-services.yml pgadmin: image: dpage/pgadmin4 ports: - \"5050:5050\" environment: - PGADMIN_DEFAULT_EMAIL=<your_email_address> - PGADMIN_DEFAULT_PASSWORD=<your_password> - PGADMIN_LISTEN_PORT=5050 docker-compose.yml pgadmin: extends: file: docker-services.yml service: pgadmin You can easily access pgAdmin 4 from your web browser. visit http://localhost:5050 . You should see the pgAdmin login page. Login with your email and password . Once you login, you should see the pgAdmin dashboard. Now, to add the PostgreSQL server running as a Docker container, right click on Servers, and then go to Create > Server\u2026 In the General tab, type in your server Name. Now, go to the Connection tab and type in db as Host name/address, (db is the container name for PostgreSQL) 5432 as Port, postgres as Maintenance database, admin as Username, secret as Password and check Save password? checkbox. Then, click on Save. NOTE: Ad Host name/address add docker container name or service name of your database. in real production it can be the IP of address of Database. pgAdmin 4 should be connected to your PostgreSQL database. Now, you can work with your PostgreSQL database as much as you want.","title":"pgAdmin"},{"location":"services/pgadmin/#pgadmin","text":"The pgAdmin package is a free and open-source graphical user interface (GUI) administration tool for PostgreSQL, which is configured as a service for your development. docker-services.yml pgadmin: image: dpage/pgadmin4 ports: - \"5050:5050\" environment: - PGADMIN_DEFAULT_EMAIL=<your_email_address> - PGADMIN_DEFAULT_PASSWORD=<your_password> - PGADMIN_LISTEN_PORT=5050 docker-compose.yml pgadmin: extends: file: docker-services.yml service: pgadmin You can easily access pgAdmin 4 from your web browser. visit http://localhost:5050 . You should see the pgAdmin login page. Login with your email and password . Once you login, you should see the pgAdmin dashboard. Now, to add the PostgreSQL server running as a Docker container, right click on Servers, and then go to Create > Server\u2026 In the General tab, type in your server Name. Now, go to the Connection tab and type in db as Host name/address, (db is the container name for PostgreSQL) 5432 as Port, postgres as Maintenance database, admin as Username, secret as Password and check Save password? checkbox. Then, click on Save. NOTE: Ad Host name/address add docker container name or service name of your database. in real production it can be the IP of address of Database. pgAdmin 4 should be connected to your PostgreSQL database. Now, you can work with your PostgreSQL database as much as you want.","title":"pgAdmin"}]}